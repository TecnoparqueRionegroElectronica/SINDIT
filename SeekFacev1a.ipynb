{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"SeekfaceAPI version 1 alpha.ipynb\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "!pip install mediapipe scikit-learn tensorflow scikeras opencv-python\n",
        "\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Initialize MediaPipe Face Mesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5\n",
        ")\n",
        "\n",
        "# Data storage\n",
        "embeddings = []\n",
        "names = []\n",
        "\n",
        "def extract_landmarks(image):\n",
        "    results = face_mesh.process(image)\n",
        "    if results.multi_face_landmarks:\n",
        "        for face_landmarks in results.multi_face_landmarks:\n",
        "            landmarks = []\n",
        "            for landmark in face_landmarks.landmark:\n",
        "                landmarks.append([landmark.x, landmark.y])\n",
        "            return landmarks\n",
        "    return None\n",
        "\n",
        "def flatten_landmarks(landmarks):\n",
        "    return [coord for point in landmarks for coord in point]\n",
        "\n",
        "def augment_image(image):\n",
        "  augmented_images = []\n",
        "  # Flip horizontally\n",
        "  augmented_images.append(cv2.flip(image, 1))\n",
        "  # Rotate 15 degrees\n",
        "  rows, cols = image.shape[:2]\n",
        "  M = cv2.getRotationMatrix2D((cols/2,rows/2), 15, 1)\n",
        "  augmented_images.append(cv2.warpAffine(image, M, (cols, rows)))\n",
        "  # Rotate -15 degrees\n",
        "  M = cv2.getRotationMatrix2D((cols/2,rows/2), -15, 1)\n",
        "  augmented_images.append(cv2.warpAffine(image, M, (cols, rows)))\n",
        "  return augmented_images"
      ],
      "metadata": {
        "id": "Ra_8mvdV3DqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HyZ7fMG43pi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKk7Rnir293J"
      },
      "outputs": [],
      "source": [
        "# --- Training Phase ---\n",
        "dataset_path = \"/content/drive/My Drive/Colab Notebooks/SINDIT/\"  # Reemplazar si cambia el lugar de ejecuci√≥n!\n",
        "for filename in os.listdir(dataset_path):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        image_path = os.path.join(dataset_path, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        landmarks = extract_landmarks(image_rgb)\n",
        "        if landmarks:\n",
        "            name = input(f\"Enter the name for the person in image {filename}: \")\n",
        "            names.append(name)\n",
        "\n",
        "            flattened_landmarks = flatten_landmarks(landmarks)\n",
        "            embeddings.append(flattened_landmarks)\n",
        "\n",
        "            # Image Augmentation\n",
        "            augmented_images = augment_image(image_rgb)\n",
        "            for augmented_image in augmented_images:\n",
        "                aug_landmarks = extract_landmarks(augmented_image)\n",
        "                if aug_landmarks:\n",
        "                    names.append(name)  # Same name for augmented images\n",
        "                    flattened_aug_landmarks = flatten_landmarks(aug_landmarks)\n",
        "                    embeddings.append(flattened_aug_landmarks)\n",
        "\n",
        "# --- Prepare data for training ---\n",
        "X = np.array(embeddings)\n",
        "y = np.array(names)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "name_to_label = {name: i for i, name in enumerate(set(names))}\n",
        "y_encoded = np.array([name_to_label[name] for name in y])\n",
        "y_onehot = to_categorical(y_encoded)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Keras Model ---\n",
        "def create_model(hidden_units=128):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(hidden_units, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "  model.add(Dense(len(set(names)), activation='softmax'))  # Output layer\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "estimator = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# --- Hyperparameter Tuning ---\n",
        "param_grid = {\n",
        "    'hidden_units': [64, 128, 256],\n",
        "    'epochs': [10, 20, 30],\n",
        "    'batch_size': [32, 64]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# --- Training ---\n",
        "history = best_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=best_model.epochs, batch_size=best_model.batch_size)\n",
        "\n",
        "# --- Plot training history ---\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# --- Testing Phase (Crowd Image) ---\n",
        "test_image_path = \"/content/crowd_test_image.jpg\"  # Replace with your crowd test image path\n",
        "test_image = cv2.imread(test_image_path)\n",
        "test_image_rgb = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "results = face_mesh.process(test_image_rgb)\n",
        "if results.multi_face_landmarks:\n",
        "    for face_landmarks in results.multi_face_landmarks:\n",
        "        landmarks = []\n",
        "        for landmark in face_landmarks.landmark:\n",
        "            x = int(landmark.x * test_image.shape[1])\n",
        "            y = int(landmark.y * test_image.shape[0])\n",
        "            landmarks.append([x, y])\n",
        "\n",
        "        flattened_landmarks = flatten_landmarks(landmarks)\n",
        "        test_data = np.array([flattened_landmarks])\n",
        "        prediction = best_model.predict(test_data)\n",
        "        predicted_class = np.argmax(prediction)\n",
        "        predicted_name = list(name_to_label.keys())[list(name_to_label.values()).index(predicted_class)]\n",
        "\n",
        "        # Draw rectangle and label on the image\n",
        "        cv2.rectangle(test_image, (landmarks[0][0], landmarks[0][1]), (landmarks[122][0], landmarks[152][1]), (0, 255, 0), 2)\n",
        "        cv2.putText(test_image, predicted_name, (landmarks[0][0], landmarks[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "cv2_imshow(test_image)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Webcam Recognition ---\n",
        "video_capture = cv2.VideoCapture(0) # 0 for default webcam\n",
        "\n",
        "while True:\n",
        "    ret, frame = video_capture.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(frame_rgb)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        for face_landmarks in results.multi_face_landmarks:\n",
        "            landmarks = []\n",
        "            for landmark in face_landmarks.landmark:\n",
        "                x = int(landmark.x * frame.shape[1])\n",
        "                y = int(landmark.y * frame.shape[0])\n",
        "                landmarks.append([x, y])\n",
        "\n",
        "            flattened_landmarks = flatten_landmarks(landmarks)\n",
        "            test_data = np.array([flattened_landmarks])\n",
        "            prediction = best_model.predict(test_data)\n",
        "            predicted_class = np.argmax(prediction)\n",
        "            predicted_name = list(name_to_label.keys())[list(name_to_label.values()).index(predicted_class)]\n",
        "\n",
        "            # Draw rectangle and label on the frame\n",
        "            cv2.rectangle(frame, (landmarks[0][0], landmarks[0][1]), (landmarks[122][0], landmarks[152][1]), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, predicted_name, (landmarks[0][0], landmarks[0][1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
        "        break\n",
        "\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "rTezJYjo38aq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}